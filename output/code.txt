package mylambda

import org.apache.log4j.{Level, Logger}
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions.{col, explode, lower, regexp_replace}


object Hellojson {def main(args: Array[String]): Unit = {

  val logger = Logger.getLogger("blah")
  Logger.getLogger("org").setLevel(Level.WARN)
  Logger.getLogger("akka").setLevel(Level.WARN)

  val spark = SparkSession.builder()
    .appName("Readjson")
    .config("spark.driver.memory", "2g")
    .master("local[*]")
    //.enableHiveSupport()
    .getOrCreate()

  import spark.implicits._

  val df = spark.read.option("multiLine", true).json("D:/hellofresh/Test/neww.json")
  df.printSchema()

  val ds = df.filter(lower($"ingredients").like("%beef%"))
    .withColumn("description",regexp_replace(col("description"),"[\\r|\\n]",""))
    .withColumn("ingredients",regexp_replace(col("ingredients"),"[\\r|\\n]",""))



  val newdf = ds.withColumn("cookTime",regexp_replace(col("cookTime"),"[a-zA-Z]","")).
    withColumn("prepTime",regexp_replace(col("prepTime"),"[a-zA-Z]","")).
    withColumn("cookTime",when(length(trim(col("cookTime")))===0,lit(0)).otherwise(col("cookTime"))).
    withColumn("prepTime",when(length(trim(col("prepTime")))===0,lit(0)).otherwise(col("prepTime")))
  .withColumn("total_cook_time",col("cookTime").cast("int") + col("prepTime").cast("int"))
  .withColumn("Difficulties", when(col("total_cook_time") < 30, lit("easy")).when((col("total_cook_time") >= 30) and (col("total_cook_time") < 60), lit("medium")).otherwise(lit("hard")))
  .groupBy("Difficulties").avg("total_cook_time")
    .coalesce(1)
    .write.option("header","true")
    .csv(path="D:/hellofresh/Test/out")



}



}
